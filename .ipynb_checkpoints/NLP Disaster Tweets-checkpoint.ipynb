{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Distaster Tweets\n",
    "Data are loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "test = pd.read_csv('./Data/test.csv')\n",
    "train = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sample_submission.shape)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The text data must be processed before we can train any models on it. First we count the occurences of words in the first five tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '13', 'alaska', 'all', 'allah', 'are', 'as', 'asked', 'being', 'by', 'california', 'canada', 'deeds', 'earthquake', 'evacuation', 'expected', 'fire', 'forest', 'forgive', 'from', 'got', 'in', 'into', 'just', 'la', 'may', 'near', 'no', 'notified', 'of', 'officers', 'or', 'orders', 'other', 'our', 'people', 'photo', 'place', 'pours', 'reason', 'receive', 'residents', 'ronge', 'ruby', 'sask', 'school', 'sent', 'shelter', 'smoke', 'the', 'this', 'to', 'us', 'wildfires']\n",
      "(5, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_text = train['text'].values\n",
    "\n",
    "# Just see what it does on the first 5 tweets\n",
    "train_text_5 = train_text[:5]\n",
    "train_text_5\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_text_5)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the `CountVectorizer` found 54 different tokens for the first five tweets. Now we will use a naive vocabulary as tokens and explore the resulting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disaster', 'fire', 'storm', 'hurricane', 'ablaze', 'rumble', 'earthquake']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7613, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['disaster', 'fire', 'storm', 'hurricane', 'ablaze', 'rumble', 'earthquake']\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "X = vectorizer.fit_transform(train_text)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "df['target'] = train['target']\n",
    "\n",
    "perc = np.zeros(df.shape[1] - 1)\n",
    "\n",
    "# Calculate percentage of tweets that are disasters for each word\n",
    "cnt = 0\n",
    "for col_nm, col_data in df.iloc[:,:-1].iteritems():\n",
    "    targets = df['target'][col_data > 0]\n",
    "    if targets.values.size == 0:\n",
    "        perc[cnt] = 0\n",
    "    else:\n",
    "        perc[cnt] = np.sum(targets.values)/targets.values.size * 100\n",
    "    cnt += 1\n",
    "\n",
    "# Plot the percentages\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(df.columns[:-1], perc)\n",
    "plt.title('Percentage of Tweets Containing Token that are a Disaster')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a baseline model that considers any tweets that includes the word disaster, fire, storm, hurricane, or earthquake a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score using the common sense baseline on the train data is 0.21422986708365907.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "disaster_vocab_df = df[['disaster', 'fire', 'storm', 'hurricane', 'earthquake']]\n",
    "pred = (disaster_vocab_df > 0).any(axis=1)\n",
    "\n",
    "f1 = f1_score(train['target'], pred)\n",
    "print(f'The f1 score using the common sense baseline on the train data is {f1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Modeling\n",
    "Now, we preprocess the data for actual predictive modeling. We will use `sklearn`'s default stop words in its `CountVectorizer`. This could be perhaps be probelmatic for reasons outlined [here](https://www.aclweb.org/anthology/W18-2502/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 21363)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(train_text)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That is a lot of features. My guess is that we do not need this many. We will try training a simple model with all the features and then go onto tuning the `TfidfVectorizer` to get the data preprocessed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58432935 0.57786614 0.56718346 0.59760956 0.62060457]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5895186150304811"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "\n",
    "# # Get data in standard format in standard variables\n",
    "# X = X.toarray()\n",
    "# y = train['target'].values\n",
    "\n",
    "# # Do cross validation on model\n",
    "# gauss_NB = GaussianNB()\n",
    "# cvs = cross_val_score(gauss_NB, X, y, scoring='f1', cv=5)\n",
    "\n",
    "# # Save Results\n",
    "# with open ('./Models/GaussianNB_cvs.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvs, f)\n",
    "    \n",
    "# Load in cross validation results\n",
    "with open('./Models/GaussianNB_cvs.pkl', 'rb') as f:\n",
    "   cvs = pickle.load(f)\n",
    "    \n",
    "print(cvs)\n",
    "np.mean(cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is much better than the common sense baseline model. Let's try some more models! We will start with random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50980392 0.3957323  0.41642789 0.4143167  0.64204046]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4756642539357637"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#rs = np.random.RandomState(42)\n",
    "\n",
    "#rfc = RandomForestClassifier(n_jobs=-1, random_state=rs, verbose=2)\n",
    "# cvs = cross_val_score(rfc, X, y, scoring='f1', verbose=2, \n",
    "#                       n_jobs=-1)\n",
    "\n",
    "# Save Results\n",
    "#with open ('./Models/RandomForestClassifer_cvs.pkl', 'wb') as f:\n",
    "#    pickle.dump(cvs, f)\n",
    "\n",
    "# Load in results\n",
    "with open ('./Models/RandomForestClassifer_cvs.pkl', 'rb') as f:\n",
    "    cvs = pickle.load(f)\n",
    "    \n",
    "print(cvs)\n",
    "np.mean(cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is worse than just guesssing. Yikes. Now we will try support vector machines. We will use a linear kernel so that it doesn't take forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58403756 0.51921435 0.58881579 0.58475336 0.69892473]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5951491578565674"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# SVC = LinearSVC(verbose=10)\n",
    "# cvs = cross_val_score(SVC, X, y, scoring='f1', verbose=10, n_jobs=-1, cv=5)\n",
    "\n",
    "# # Save Results\n",
    "# with open('./Models/LinearSVC_cvs.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvs, f)\n",
    "    \n",
    "# Load in results\n",
    "with open ('./Models/LinearSVC_cvs.pkl', 'rb') as f:\n",
    "    cvs = pickle.load(f)\n",
    "    \n",
    "print(cvs)\n",
    "np.mean(cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This is the best result yet. I think there is great promise in using a Support Vector Machine. Let's go back and preprocess the data little better and then try modeling again. We will also tune our models' hyperparameters on the second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crash', 'suicide', 'california', 'burning', 'storm', 'body',\n",
       "       'police', 'disaster', 'emergency', 'video', 'don', 'news',\n",
       "       'people', 'new', 'just', 'amp', 'û_', 'like', 'https', 'http'],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First lets find the most common occuring words\n",
    "\n",
    "# Count words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# Get the total counts of each word\n",
    "cnt_tot = X.toarray().sum(axis=0)\n",
    "\n",
    "# Print the top 20 most occuring words\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "vocab = vocab[np.argsort(cnt_tot)]\n",
    "vocab_top20 = vocab[-20:]\n",
    "\n",
    "vocab_top20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the most commonly occuring vocab is fairly relevant. However, there are some things like 'https' which are not so relvant. Hopefully inverse document frequency will take care of such words. We continue by lemmatizing the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mboli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full vocab had 21363 words when the lemmatized vocab has 20173 words.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vocab_lemmatized = [lemmatizer.lemmatize(word) for word in vocab]\n",
    "vocab_lemmatized = np.array(vocab_lemmatized)\n",
    "vocab_lemmatized = np.unique(vocab_lemmatized)\n",
    "\n",
    "print(f'The full vocab had {vocab.size} words when the lemmatized vocab has {vocab_lemmatized.size} words.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
